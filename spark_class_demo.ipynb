{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a139480",
   "metadata": {},
   "source": [
    "# NYC Taxi Data Analysis with PySpark\n",
    "\n",
    "**Important:** This notebook requires Python 3.11 or 3.12. Python 3.13 is not compatible with PySpark due to Java security manager changes.\n",
    "\n",
    "If you see the error `getSubject is supported only if a security manager is allowed`, you need to:\n",
    "1. Select a Python 3.12 kernel: Click on the kernel selector in the top right\n",
    "2. Choose \"Select Another Kernel\" â†’ \"Python Environments\"\n",
    "3. Select a Python 3.12 interpreter (not 3.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fbc839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2023-02-01 00:32:53   2023-02-01 00:34:34              2.0   \n",
      "1         2  2023-02-01 00:35:16   2023-02-01 00:35:30              1.0   \n",
      "2         2  2023-02-01 00:35:16   2023-02-01 00:35:30              1.0   \n",
      "3         1  2023-02-01 00:29:33   2023-02-01 01:01:38              0.0   \n",
      "4         2  2023-02-01 00:12:28   2023-02-01 00:25:46              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           0.30         1.0                  N           142           163   \n",
      "1           0.00         1.0                  N            71            71   \n",
      "2           0.00         1.0                  N            71            71   \n",
      "3          18.80         1.0                  N           132            26   \n",
      "4           3.22         1.0                  N           161           145   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2          4.4   3.50      0.5         0.0           0.0   \n",
      "1             4         -3.0  -1.00     -0.5         0.0           0.0   \n",
      "2             4          3.0   1.00      0.5         0.0           0.0   \n",
      "3             1         70.9   2.25      0.5         0.0           0.0   \n",
      "4             1         17.0   1.00      0.5         3.3           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
      "0                    1.0          9.40                   2.5         0.00  \n",
      "1                   -1.0         -5.50                   0.0         0.00  \n",
      "2                    1.0          5.50                   0.0         0.00  \n",
      "3                    1.0         74.65                   0.0         1.25  \n",
      "4                    1.0         25.30                   2.5         0.00  \n",
      "pyarrow.Table\n",
      "VendorID: int32\n",
      "tpep_pickup_datetime: timestamp[us]\n",
      "tpep_dropoff_datetime: timestamp[us]\n",
      "passenger_count: int64\n",
      "trip_distance: double\n",
      "RatecodeID: int64\n",
      "store_and_fwd_flag: large_string\n",
      "PULocationID: int32\n",
      "DOLocationID: int32\n",
      "payment_type: int64\n",
      "fare_amount: double\n",
      "extra: double\n",
      "mta_tax: double\n",
      "tip_amount: double\n",
      "tolls_amount: double\n",
      "improvement_surcharge: double\n",
      "total_amount: double\n",
      "congestion_surcharge: double\n",
      "Airport_fee: double\n",
      "----\n",
      "VendorID: [[1,2,2,1,2,...,1,2,2,2,2],[2,1,1,1,1,...,2,2,2,2,2],...,[2,2,2,1,1,...,2,2,2,2,2],[1,2,2,2,2,...,2,2,2,2,2]]\n",
      "tpep_pickup_datetime: [[2023-02-01 00:32:53.000000,2023-02-01 00:35:16.000000,2023-02-01 00:35:16.000000,2023-02-01 00:29:33.000000,2023-02-01 00:12:28.000000,...,2023-02-02 11:45:12.000000,2023-02-02 11:03:31.000000,2023-02-02 11:03:42.000000,2023-02-02 11:14:37.000000,2023-02-02 11:18:45.000000],[2023-02-02 11:22:34.000000,2023-02-02 11:43:19.000000,2023-02-02 11:59:39.000000,2023-02-02 11:40:25.000000,2023-02-02 11:54:29.000000,...,2023-02-03 14:47:34.000000,2023-02-03 14:27:22.000000,2023-02-03 14:46:21.000000,2023-02-03 13:57:43.000000,2023-02-03 14:27:32.000000],...,[2023-02-28 09:27:43.000000,2023-02-28 09:04:35.000000,2023-02-28 09:13:25.000000,2023-02-28 09:06:22.000000,2023-02-28 09:53:58.000000,...,2023-02-17 11:21:16.000000,2023-02-17 11:11:17.000000,2023-02-17 11:32:13.000000,2023-02-17 11:03:00.000000,2023-02-17 11:29:00.000000],[2023-02-17 11:33:49.000000,2023-02-17 11:57:59.000000,2023-02-17 11:29:09.000000,2023-02-17 11:53:52.000000,2023-02-17 11:15:55.000000,...,2023-02-28 23:46:00.000000,2023-02-28 23:26:02.000000,2023-02-28 23:24:00.000000,2023-02-28 23:03:00.000000,2023-02-28 23:03:03.000000]]\n",
      "tpep_dropoff_datetime: [[2023-02-01 00:34:34.000000,2023-02-01 00:35:30.000000,2023-02-01 00:35:30.000000,2023-02-01 01:01:38.000000,2023-02-01 00:25:46.000000,...,2023-02-02 12:06:28.000000,2023-02-02 11:10:52.000000,2023-02-02 11:12:50.000000,2023-02-02 11:28:23.000000,2023-02-02 11:50:31.000000],[2023-02-02 11:39:57.000000,2023-02-02 11:51:07.000000,2023-02-02 12:18:00.000000,2023-02-02 12:01:08.000000,2023-02-02 12:27:12.000000,...,2023-02-03 15:22:17.000000,2023-02-03 14:38:57.000000,2023-02-03 15:35:58.000000,2023-02-03 14:20:17.000000,2023-02-03 14:35:16.000000],...,[2023-02-28 09:48:39.000000,2023-02-28 09:10:07.000000,2023-02-28 09:46:49.000000,2023-02-28 09:29:42.000000,2023-02-28 10:07:00.000000,...,2023-02-17 11:56:17.000000,2023-02-17 11:32:26.000000,2023-02-17 11:55:19.000000,2023-02-17 11:16:00.000000,2023-02-17 11:48:00.000000],[2023-02-17 11:44:58.000000,2023-02-17 12:40:32.000000,2023-02-17 11:35:38.000000,2023-02-17 12:05:43.000000,2023-02-17 11:29:38.000000,...,2023-03-01 00:05:00.000000,2023-02-28 23:37:10.000000,2023-02-28 23:38:00.000000,2023-02-28 23:10:00.000000,2023-02-28 23:12:51.000000]]\n",
      "passenger_count: [[2,1,1,0,1,...,1,1,1,2,4],[1,1,1,1,2,...,2,1,1,1,1],...,[1,1,1,1,1,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\n",
      "trip_distance: [[0.3,0,0,18.8,3.22,...,2.7,0.76,4.2,2.4,11.84],[2.01,1.1,2.6,1.7,12.7,...,9.35,2.62,17.06,2.88,1.01],...,[2.65,0.43,4.29,1.8,2,...,5.16,12.01,2.66,1.68,4],[1.2,6.6,0.22,0.77,1.79,...,4.65,2.47,3.49,2.13,2.28]]\n",
      "RatecodeID: [[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,5,...,1,1,3,1,1],...,[1,1,1,1,1,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\n",
      "store_and_fwd_flag: [[\"N\",\"N\",\"N\",\"N\",\"N\",...,\"N\",\"N\",\"N\",\"N\",\"N\"],[\"N\",\"N\",\"N\",\"N\",\"N\",...,\"N\",\"N\",\"N\",\"N\",\"N\"],...,[\"N\",\"N\",\"N\",\"N\",\"N\",...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\n",
      "PULocationID: [[142,71,71,132,161,...,162,263,87,233,132],[229,24,238,161,70,...,138,107,87,236,234],...,[141,125,158,237,238,...,249,63,162,113,232],[142,140,163,164,25,...,249,186,158,79,161]]\n",
      "DOLocationID: [[163,71,71,26,145,...,211,141,233,140,71],[236,151,162,137,265,...,48,232,1,186,68],...,[100,158,236,48,237,...,263,138,249,148,164],[163,82,163,68,195,...,140,79,143,162,140]]\n",
      "payment_type: [[2,4,4,1,1,...,1,1,2,1,1],[1,1,1,1,1,...,2,1,1,1,1],...,[1,2,1,1,1,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq_df = pd.read_parquet(\"data/raw/yellow_tripdata_2023-02.parquet\")\n",
    "print(pq_df.head())\n",
    "\n",
    "table = pq.read_table(\"data/raw/yellow_tripdata_2023-02.parquet\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24131c11-9209-4cfa-968e-06abedd247e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version detected: \"23.0.2\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# CRITICAL FIX for Java 21+ compatibility with PySpark\n",
    "# Java 21+ removed security manager, need to enable it with special flag\n",
    "# Must be set BEFORE any PySpark imports\n",
    "import subprocess\n",
    "\n",
    "# Check if JAVA_HOME is set, if not find Java location\n",
    "java_cmd = \"java\"\n",
    "try:\n",
    "    result = subprocess.run([java_cmd, \"-version\"], capture_output=True, text=True)\n",
    "    print(f\"Java version detected: {result.stderr.split()[2]}\")\n",
    "except:\n",
    "    print(\"Could not detect Java version\")\n",
    "\n",
    "# Set environment variables to enable security manager for Java 21+\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--driver-java-options '-Djava.security.manager=allow' pyspark-shell\"\n",
    ")\n",
    "os.environ[\"SPARK_SUBMIT_OPTS\"] = \"-Djava.security.manager=allow\"\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    DoubleType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    TimestampNTZType,\n",
    ")\n",
    "\n",
    "taxi_schema = StructType(\n",
    "    [\n",
    "        StructField(\"VendorID\", LongType(), True),\n",
    "        StructField(\"tpep_pickup_datetime\", TimestampNTZType(), True),\n",
    "        StructField(\"tpep_dropoff_datetime\", TimestampNTZType(), True),\n",
    "        StructField(\"passenger_count\", LongType(), True),\n",
    "        StructField(\"trip_distance\", DoubleType(), True),\n",
    "        StructField(\"RatecodeID\", LongType(), True),\n",
    "        StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "        StructField(\"PULocationID\", LongType(), True),\n",
    "        StructField(\"DOLocationID\", LongType(), True),\n",
    "        StructField(\"payment_type\", LongType(), True),\n",
    "        StructField(\"fare_amount\", DoubleType(), True),\n",
    "        StructField(\"extra\", DoubleType(), True),\n",
    "        StructField(\"mta_tax\", DoubleType(), True),\n",
    "        StructField(\"tip_amount\", DoubleType(), True),\n",
    "        StructField(\"tolls_amount\", DoubleType(), True),\n",
    "        StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "        StructField(\"total_amount\", DoubleType(), True),\n",
    "        StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "        StructField(\"airport_fee\", DoubleType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Spark session with explicit Java options for Java 21+ compatibility\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Lecture-Demo\")\n",
    "    .master(\"local[4]\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.security.manager=allow\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "file_paths = [\n",
    "    \"data/raw/yellow_tripdata_2023-02.parquet\",\n",
    "    \"data/raw/yellow_tripdata_2023-03.parquet\",\n",
    "]\n",
    "df = spark.read.schema(taxi_schema).parquet(*file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00dbf395-2ea6-41c0-8cc9-622956132a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (7)\n",
      "+- HashAggregate (6)\n",
      "   +- Exchange (5)\n",
      "      +- HashAggregate (4)\n",
      "         +- Project (3)\n",
      "            +- Filter (2)\n",
      "               +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [2]: [passenger_count#3L, PULocationID#7L]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/c:/Users/adurs/OneDrive/Documents/repos/WashU/CSE 5114 - Data Manipulation/Assignments/5 - Spark/CSE5114-spark-nyc-taxi/data/raw/yellow_tripdata_2023-02.parquet, ... 1 entries]\n",
      "PushedFilters: [IsNotNull(passenger_count), GreaterThan(passenger_count,0)]\n",
      "ReadSchema: struct<passenger_count:bigint,PULocationID:bigint>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [passenger_count#3L, PULocationID#7L]\n",
      "Condition : (isnotnull(passenger_count#3L) AND (passenger_count#3L > 0))\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [PULocationID#7L]\n",
      "Input [2]: [passenger_count#3L, PULocationID#7L]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [1]: [PULocationID#7L]\n",
      "Keys [1]: [PULocationID#7L]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#41L]\n",
      "Results [2]: [PULocationID#7L, count#42L]\n",
      "\n",
      "(5) Exchange\n",
      "Input [2]: [PULocationID#7L, count#42L]\n",
      "Arguments: hashpartitioning(PULocationID#7L, 200), ENSURE_REQUIREMENTS, [plan_id=15]\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [PULocationID#7L, count#42L]\n",
      "Keys [1]: [PULocationID#7L]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#40L]\n",
      "Results [2]: [PULocationID#7L, count(1)#40L AS count#20L]\n",
      "\n",
      "(7) AdaptiveSparkPlan\n",
      "Output [2]: [PULocationID#7L, count#20L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plan = df.filter(F.col(\"passenger_count\") > 0).groupBy(\"PULocationID\").count()\n",
    "plan.explain(\"formatted\")  # Nothing runs yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f3be2b-94dc-45ab-b117-768a9b6dd958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|PULocationID| count|\n",
      "+------------+------+\n",
      "|         132|297868|\n",
      "|         161|286788|\n",
      "|         237|278842|\n",
      "|         236|254010|\n",
      "|         162|220572|\n",
      "+------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "plan.orderBy(F.desc(\"count\")).show(5)  # Action => triggers a Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7879579-a9c8-43d2-b73d-1ebc5e7e37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [PULocationID#7L]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/c:/Users/adurs/OneDrive/Documents/repos/WashU/CSE 5114 - Data Manipulation/Assignments/5 - Spark/CSE5114-spark-nyc-taxi/data/raw/yellow_tripdata_2023-02.parquet, ... 1 entries]\n",
      "ReadSchema: struct<PULocationID:bigint>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [PULocationID#7L]\n",
      "Keys [1]: [PULocationID#7L]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#71L]\n",
      "Results [2]: [PULocationID#7L, count#72L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [PULocationID#7L, count#72L]\n",
      "Arguments: hashpartitioning(PULocationID#7L, 8), ENSURE_REQUIREMENTS, [plan_id=95]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [2]: [PULocationID#7L, count#72L]\n",
      "Keys [1]: [PULocationID#7L]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#70L]\n",
      "Results [2]: [PULocationID#7L, count(1)#70L AS count#50L]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [2]: [PULocationID#7L, count#50L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "+------------+------+\n",
      "|PULocationID| count|\n",
      "+------------+------+\n",
      "|         132|301939|\n",
      "|         161|296284|\n",
      "|         237|288885|\n",
      "|         236|265758|\n",
      "|         162|227127|\n",
      "|         186|220284|\n",
      "|         230|214893|\n",
      "|         142|203117|\n",
      "|         138|199912|\n",
      "|         170|187014|\n",
      "+------------+------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "by_zone = df.groupBy(\"PULocationID\").count()\n",
    "by_zone.explain(\"formatted\")  # Look for Exchange (shuffle) node\n",
    "by_zone.orderBy(F.desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3082c7-b34f-4c17-9733-4d574cc2beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.select(\n",
    "    F.col(\"tpep_pickup_datetime\").alias(\"pickup_ts\"),\n",
    "    F.col(\"passenger_count\").cast(\"int\").alias(\"passengers\"),\n",
    "    F.col(\"PULocationID\").cast(\"int\").alias(\"PU\"),\n",
    "    F.col(\"DOLocationID\").cast(\"int\").alias(\"DO\"),\n",
    "    F.col(\"total_amount\").cast(\"double\").alias(\"total\"),\n",
    ").filter((F.col(\"passengers\") > 0) & (F.col(\"total\") >= 0))\n",
    "\n",
    "df_clean.createOrReplaceTempView(\"trips_clean\")\n",
    "\n",
    "zones = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .csv(\"data/raw/taxi_zone_lookup.csv\")\n",
    "    .select(\n",
    "        F.col(\"LocationID\").cast(\"int\").alias(\"LocationID\"),\n",
    "        F.col(\"Borough\"),\n",
    "        F.col(\"Zone\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abd983e-8006-4bb5-a283-69f4f4251efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| PU|             PU_Zone|total|\n",
      "+---+--------------------+-----+\n",
      "|142| Lincoln Square East|  9.4|\n",
      "| 71|East Flatbush/Far...|  5.5|\n",
      "|161|      Midtown Center| 25.3|\n",
      "|148|     Lower East Side|32.25|\n",
      "|137|            Kips Bay| 50.0|\n",
      "+---+--------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "enriched = (\n",
    "    df_clean.join(F.broadcast(zones), df_clean.PU == zones.LocationID, \"left\")\n",
    "    .drop(\"LocationID\")\n",
    "    .withColumnRenamed(\"Zone\", \"PU_Zone\")\n",
    ")\n",
    "enriched.select(\"PU\", \"PU_Zone\", \"total\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7d52b2-c7fa-4b5f-8a1c-2cec74d6c7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = df_clean.filter(F.col(\"PU\") == 1)\n",
    "subset_cached = subset.cache()\n",
    "subset_cached.count()  # materialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d583c56-d5aa-4001-b7e8-cf8d8ec667df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_ts: timestamp_ntz, passengers: int, PU: int, DO: int, total: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reused multiple times\n",
    "subset_cached.groupBy(\"DO\").count().count()\n",
    "subset_cached.agg(F.sum(\"total\")).collect()\n",
    "subset_cached.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac7a8cc-8516-430f-b9d3-34f2de9acf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| PU| trips|\n",
      "+---+------+\n",
      "|132|293087|\n",
      "|161|284623|\n",
      "|237|276938|\n",
      "|236|252419|\n",
      "|162|218919|\n",
      "|186|212418|\n",
      "|230|206365|\n",
      "|138|195118|\n",
      "|142|193450|\n",
      "|170|178950|\n",
      "+---+------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "_ = (\n",
    "    df_clean.groupBy(\"PU\")\n",
    "    .agg(F.count(\"*\").alias(\"trips\"))\n",
    "    .orderBy(F.desc(\"trips\"))\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666870a9-e2ba-46fb-b533-1ddd0c0e46b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.84s | Tuned: 0.64s (lower is better)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "start = time.perf_counter()\n",
    "_ = df_clean.groupBy(\"PU\").agg(F.sum(\"total\").alias(\"revenue\")).count()\n",
    "t1 = time.perf_counter() - start\n",
    "\n",
    "# Example tweak: increase partitions if underutilized\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "start = time.perf_counter()\n",
    "_ = df_clean.groupBy(\"PU\").agg(F.sum(\"total\").alias(\"revenue\")).count()\n",
    "t2 = time.perf_counter() - start\n",
    "\n",
    "print(f\"Baseline: {t1:.2f}s | Tuned: {t2:.2f}s (lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1bc6d6-2a02-4eea-8c0a-2a762b31d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
